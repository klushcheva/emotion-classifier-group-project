# -*- coding: utf-8 -*-
"""project_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HmBZZV6HU3OXpqLgoHC3r883d7NF55xx
"""

!pip install transformers
!pip install torch

!pip install --upgrade transformers
!pip install --upgrade safetensors

import pandas as pd
from google.colab import drive

from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl

import torch
from torch.utils.data import Dataset

from sklearn.metrics import classification_report

drive.mount('/content/drive')

filename_date_marked = '/content/drive/MyDrive/Проект/posts_vk_10000_marked.xlsx'
df_sorted = pd.read_excel(filename_date_marked)

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer
from torch.utils.data import Dataset
import torch

# Загрузка данных
labeled_df = df_sorted

# Разделение данных на обучающую, валидационную и тестовую выборки
train_texts, val_test_texts, train_labels, val_test_labels = train_test_split(
    labeled_df['post'], labeled_df['tonality'], test_size=0.4, random_state=42, stratify=labeled_df['tonality']
)

val_texts, test_texts, val_labels, test_labels = train_test_split(
    val_test_texts, val_test_labels, test_size=0.5, random_state=42, stratify=val_test_labels
)

# Токенизация текстов
tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')
train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)

# Преобразование меток в числовой формат
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
train_labels = train_labels.map(label_mapping).astype(int).tolist()
val_labels = val_labels.map(label_mapping).astype(int).tolist()
test_labels = test_labels.map(label_mapping).astype(int).tolist()

# Класс Dataset
class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]).contiguous() for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long).contiguous()
        return item

    def __len__(self):
        return len(self.labels)

# Создание датасетов
train_dataset = SentimentDataset(train_encodings, train_labels)
val_dataset = SentimentDataset(val_encodings, val_labels)
test_dataset = SentimentDataset(test_encodings, test_labels)

# Тренировочные аргументы
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=1e-5,
    warmup_steps=500,
    weight_decay=0.2,
    logging_dir='./logs',
    logging_steps=50,
    evaluation_strategy="epoch",
    save_strategy="no",  # Отключаем промежуточное сохранение
    gradient_accumulation_steps=2,
    max_grad_norm=1.0
)

# Класс модели
from transformers import BertModel
import torch.nn as nn

class BertSentimentAnalyzer(nn.Module):
    def __init__(self):
        super(BertSentimentAnalyzer, self).__init__()
        self.bert = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')
        self.num_labels = 3
        self.fc_classification = nn.Linear(self.bert.config.hidden_size, self.num_labels)

    def forward(self, input_ids, attention_mask, labels=None):
        input_ids = input_ids.contiguous()
        attention_mask = attention_mask.contiguous()
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = bert_output.pooler_output
        pooled_output = pooled_output.contiguous()
        logits = self.fc_classification(pooled_output)

        if labels is not None:
            labels = labels.contiguous()
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)
            return (loss, logits)
        return logits

model = BertSentimentAnalyzer()
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = model.to(device)

# Инициализация Trainer без параметра `use_safetensors`
trainer = Trainer(
    model=model,
    data_collator=lambda data: {
        'input_ids': torch.stack([f['input_ids'] for f in data]).contiguous(),
        'attention_mask': torch.stack([f['attention_mask'] for f in data]).contiguous(),
        'labels': torch.tensor([f['labels'] for f in data], dtype=torch.long).contiguous()
    },
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Запуск обучения
trainer.train()

# Оценка метрик на тестовой выборке
from sklearn.metrics import classification_report

# Получение предсказаний на тестовом наборе данных
predictions = trainer.predict(test_dataset)
preds = predictions.predictions.argmax(axis=1)

# Оценка метрик на тестовой выборке
print(classification_report(test_labels, preds, target_names=['negative', 'neutral', 'positive']))

from transformers import BertConfig

# Создание конфигурации на основе предобученной модели
config = BertConfig.from_pretrained(
    'DeepPavlov/rubert-base-cased',  # Используем предобученную модель
    num_labels=3  # Количество классов для классификации
)

# Сохранение конфигурации
config.save_pretrained('/content/drive/MyDrive/Проект/Модель')

print(config)

import os
import torch
import shutil
from transformers import BertConfig

# Путь для сохранения
model_save_path = '/content/drive/MyDrive/Проект/Модель'

# Удаление старой директории, если она существует
if os.path.exists(model_save_path):
    shutil.rmtree(model_save_path)

# Создание новой директории
os.makedirs(model_save_path, exist_ok=True)

# Сохранение весов модели
torch.save(model.state_dict(), os.path.join(model_save_path, 'pytorch_model.bin'))

# Сохранение конфигурации, если она отличается от стандартной
config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=3)  # укажи конфигурацию, которая соответствует твоей модели
config.save_pretrained(model_save_path)

# Сохранение токенизатора
tokenizer.save_pretrained(model_save_path)

# Пример текста для предсказания
text = "Ура! Ура! Ура! Поздравляем победителей от души!!!"

# Токенизация
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

# Удаляем 'token_type_ids', если модель его не использует
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']

# Перемещаем входные данные на устройство (GPU или CPU)
inputs = {key: val.to(device) for key, val in inputs.items()}

# Прогон через модель
model.eval()
with torch.no_grad():
    outputs = model(**inputs)

# Получение предсказания
predictions = torch.argmax(outputs, dim=-1)
print(f"Предсказанный класс: {predictions.item()}")